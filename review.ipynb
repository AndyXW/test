{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[ 0.3058, -1.2311,  0.2594],\n",
      "        [ 0.3707, -0.6336,  1.1749],\n",
      "        [-0.4051,  0.4016,  1.1909],\n",
      "        [ 0.4499, -0.3473,  0.1786],\n",
      "        [-1.3147,  0.2483, -0.0051]])\n",
      "torch.Size([5, 3])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.empty(5, 3)\n",
    "x = torch.rand(5, 3)\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "x = torch.tensor([5.5, 3])\n",
    "x = x.new_ones(5, 3, dtype=torch.double)\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.6260, 0.6784, 0.1697],\n        [0.4688, 0.5834, 0.0897],\n        [0.2204, 0.0518, 0.2144],\n        [0.5400, 0.0700, 0.6336],\n        [0.5112, 0.9123, 0.3233]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 18
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[ 0.9318, -0.5528,  0.4292],\n",
      "        [ 0.8395, -0.0502,  1.2646],\n",
      "        [-0.1847,  0.4534,  1.4053],\n",
      "        [ 0.9899, -0.2773,  0.8122],\n",
      "        [-0.8035,  1.1606,  0.3182]])\n",
      "tensor([-1.2311, -0.6336,  0.4016, -0.3473,  0.2483])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(torch.add(x, y))\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "# any operation that mutates a tensor in-place is post-fixed\n",
    "# with an _ . For example: x.copy_(y), x.t_()\n",
    "# will change x\n",
    "y.add_(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[-0.0567, -0.9079,  0.3804, -0.5696],\n",
      "        [ 1.0613,  1.0620, -0.3570,  0.1030]])\n",
      "torch.Size([4, 4]) torch.Size([2, 8]) torch.Size([2, 2, 4])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(x[:, 1])\n",
    "# if you want to resize/reshape tensor, you can use torch.view:\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(-1, 8)\n",
    "z = x.view(-1, 2, 4)\n",
    "print(x.size(), y.size(), z.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([2.0661])\n",
      "2.066114902496338\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# if you have a one element tensor, use.item() to get the value as a Python number\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0 31746839.663173623\n",
      "1 29398788.327436335\n",
      "2 31696752.49791666\n",
      "3 33534076.274362758\n",
      "4 30425900.396040745\n",
      "5 22427780.52101308\n",
      "6 13270342.343572147\n",
      "7 6914569.617021697\n",
      "8 3532537.4693149813\n",
      "9 1980931.761589724\n",
      "10 1270164.2712220363\n",
      "11 919696.1258223027\n",
      "12 722114.6667584493\n",
      "13 594321.935534439\n",
      "14 501956.11641762045\n",
      "15 430433.68474133906\n",
      "16 372620.931260184\n",
      "17 324711.28991782246\n",
      "18 284408.5594293276\n",
      "19 250238.27534570522\n",
      "20 220990.29355209836\n",
      "21 195842.15881316035\n",
      "22 174134.2653017016\n",
      "23 155323.79372610466\n",
      "24 138929.54651294844\n",
      "25 124583.26730077242\n",
      "26 111984.85013356383\n",
      "27 100884.39760766384\n",
      "28 91077.2661092757\n",
      "29 82384.45700601439\n",
      "30 74658.97209072708\n",
      "31 67781.54944504393\n",
      "32 61646.32651235578\n",
      "33 56159.25341929644\n",
      "34 51240.27200974333\n",
      "35 46821.09796645605\n",
      "36 42842.14329868824\n",
      "37 39253.43368063192\n",
      "38 36012.087007156515\n",
      "39 33079.810436825486\n",
      "40 30423.14922461459\n",
      "41 28010.75419246264\n",
      "42 25818.6438686241\n",
      "43 23822.259531844164\n",
      "44 22002.057492072523\n",
      "45 20341.2997037104\n",
      "46 18824.194370784255\n",
      "47 17436.093710181005\n",
      "48 16164.000864050127\n",
      "49 14997.383598674724\n",
      "50 13928.111486159138\n",
      "51 12945.757750197114\n",
      "52 12042.029601782324\n",
      "53 11210.73623956515\n",
      "54 10444.92409642838\n",
      "55 9737.951537036166\n",
      "56 9085.376879268551\n",
      "57 8482.322774871114\n",
      "58 7924.16847520269\n",
      "59 7407.427890558275\n",
      "60 6928.7873591938005\n",
      "61 6484.663406513433\n",
      "62 6072.454233536841\n",
      "63 5689.4450421699\n",
      "64 5333.581709995435\n",
      "65 5002.624454344823\n",
      "66 4694.54093146814\n",
      "67 4407.698112103947\n",
      "68 4140.360294942453\n",
      "69 3891.0006047832635\n",
      "70 3658.373329742427\n",
      "71 3441.2752576964867\n",
      "72 3238.4064883356778\n",
      "73 3048.8693208799364\n",
      "74 2871.6149034855935\n",
      "75 2705.8809630329206\n",
      "76 2550.568479185824\n",
      "77 2405.152732598139\n",
      "78 2268.8536707752405\n",
      "79 2141.0827768470417\n",
      "80 2021.2503106671295\n",
      "81 1908.7904653825435\n",
      "82 1803.2950977657044\n",
      "83 1704.1431016483107\n",
      "84 1610.956725531311\n",
      "85 1523.3810706818208\n",
      "86 1441.0444894711056\n",
      "87 1363.611072900272\n",
      "88 1290.695746347371\n",
      "89 1222.058511623319\n",
      "90 1157.446580248843\n",
      "91 1096.5662973894468\n",
      "92 1039.1537170251852\n",
      "93 985.028356370233\n",
      "94 933.9828705350116\n",
      "95 885.8264778428083\n",
      "96 840.3594268021778\n",
      "97 797.4313022980621\n",
      "98 756.9086472213294\n",
      "99 718.6123550740008\n",
      "100 682.4214577594403\n",
      "101 648.2085725337575\n",
      "102 615.8504800436046\n",
      "103 585.2260141549068\n",
      "104 556.2569544235984\n",
      "105 528.8454665764339\n",
      "106 502.89168998943967\n",
      "107 478.30654868918407\n",
      "108 455.02083411245724\n",
      "109 432.95974266331694\n",
      "110 412.05208411911354\n",
      "111 392.2150017952889\n",
      "112 373.4053983081866\n",
      "113 355.5720529564019\n",
      "114 338.65355727698625\n",
      "115 322.5994888305189\n",
      "116 307.3597417412343\n",
      "117 292.89252158691636\n",
      "118 279.15419970384016\n",
      "119 266.1053283060535\n",
      "120 253.70140292629338\n",
      "121 241.92035705905346\n",
      "122 230.71661415017377\n",
      "123 220.06841589590536\n",
      "124 209.94706800657522\n",
      "125 200.31989357053635\n",
      "126 191.16172991939746\n",
      "127 182.45555264307683\n",
      "128 174.16289897506775\n",
      "129 166.2700120444605\n",
      "130 158.75658977771968\n",
      "131 151.60375302271802\n",
      "132 144.79441045573697\n",
      "133 138.31029000795638\n",
      "134 132.12887938582327\n",
      "135 126.2429910617832\n",
      "136 120.6331654220001\n",
      "137 115.28558647715343\n",
      "138 110.18691585816684\n",
      "139 105.3272007989205\n",
      "140 100.69192001106376\n",
      "141 96.27243741294963\n",
      "142 92.05647254367327\n",
      "143 88.035374957443\n",
      "144 84.19861425820561\n",
      "145 80.53711377537874\n",
      "146 77.04206946108175\n",
      "147 73.7070472669708\n",
      "148 70.5222875054846\n",
      "149 67.48320500336096\n",
      "150 64.57970747241832\n",
      "151 61.80886047226718\n",
      "152 59.16159940158975\n",
      "153 56.632531225592146\n",
      "154 54.21545178880443\n",
      "155 51.906375537667756\n",
      "156 49.699654096572296\n",
      "157 47.59095745369365\n",
      "158 45.57656430953354\n",
      "159 43.64957987865431\n",
      "160 41.80794113983167\n",
      "161 40.04764819391374\n",
      "162 38.36424480733142\n",
      "163 36.75345433102599\n",
      "164 35.21371897237701\n",
      "165 33.741616771191374\n",
      "166 32.331998181523275\n",
      "167 30.98386330452918\n",
      "168 29.693958305365605\n",
      "169 28.459910992601895\n",
      "170 27.278831780980568\n",
      "171 26.148880268685794\n",
      "172 25.067292424331058\n",
      "173 24.03137926882566\n",
      "174 23.039749819232405\n",
      "175 22.09066546952651\n",
      "176 21.18166613307816\n",
      "177 20.31152397046896\n",
      "178 19.4784684770789\n",
      "179 18.680682424120043\n",
      "180 17.91656347224121\n",
      "181 17.184572430781166\n",
      "182 16.48349212091039\n",
      "183 15.811699721727125\n",
      "184 15.167980762615349\n",
      "185 14.5515574530542\n",
      "186 13.960577920454043\n",
      "187 13.394166168799057\n",
      "188 12.851504598529647\n",
      "189 12.331548531824389\n",
      "190 11.832983681891733\n",
      "191 11.35525764279501\n",
      "192 10.897614213606658\n",
      "193 10.458651279795147\n",
      "194 10.037783981049067\n",
      "195 9.634264359921396\n",
      "196 9.247596529623063\n",
      "197 8.87657395670032\n",
      "198 8.52089151809001\n",
      "199 8.179836999210815\n",
      "200 7.852777305153667\n",
      "201 7.5389045683753295\n",
      "202 7.237877759074064\n",
      "203 6.949242422225419\n",
      "204 6.672429647087647\n",
      "205 6.406891652884253\n",
      "206 6.152084592736353\n",
      "207 5.9076337800779175\n",
      "208 5.6730463595418\n",
      "209 5.44799989809899\n",
      "210 5.232123374863732\n",
      "211 5.025019460005573\n",
      "212 4.8262660022267845\n",
      "213 4.6354708575702634\n",
      "214 4.4523726435616275\n",
      "215 4.276627890327048\n",
      "216 4.107955128683006\n",
      "217 3.9461361173725145\n",
      "218 3.790736784660191\n",
      "219 3.6415525662295143\n",
      "220 3.498386176421208\n",
      "221 3.3609042058587417\n",
      "222 3.2289141976619984\n",
      "223 3.1022762198432723\n",
      "224 2.9806763074625433\n",
      "225 2.8639021570356804\n",
      "226 2.751770992684219\n",
      "227 2.644117378855541\n",
      "228 2.5407376487646\n",
      "229 2.4414796045173848\n",
      "230 2.3461817717970024\n",
      "231 2.2546064704268876\n",
      "232 2.166676982395674\n",
      "233 2.082257390887393\n",
      "234 2.0011419658685647\n",
      "235 1.9232605416348427\n",
      "236 1.8484501275654797\n",
      "237 1.7765694495241515\n",
      "238 1.7075135131855155\n",
      "239 1.6411886678295478\n",
      "240 1.5775171817226292\n",
      "241 1.5163467045554544\n",
      "242 1.4575770930758989\n",
      "243 1.401111296623485\n",
      "244 1.3468552351530643\n",
      "245 1.2947102436493787\n",
      "246 1.2446296107612442\n",
      "247 1.1965103234670582\n",
      "248 1.1502852733699023\n",
      "249 1.105856867143409\n",
      "250 1.0631563307634835\n",
      "251 1.0221271154576208\n",
      "252 0.9827110279094795\n",
      "253 0.9448440144296151\n",
      "254 0.9084422227368829\n",
      "255 0.8734502698244526\n",
      "256 0.8398350691332104\n",
      "257 0.8075175546903738\n",
      "258 0.7764635672885905\n",
      "259 0.7466344424705604\n",
      "260 0.7179602385718336\n",
      "261 0.6903854401254788\n",
      "262 0.6638737288116388\n",
      "263 0.6383931393459348\n",
      "264 0.6139052690485904\n",
      "265 0.590384584584773\n",
      "266 0.5677673603006552\n",
      "267 0.546016281776038\n",
      "268 0.5251049735384468\n",
      "269 0.5050066360988815\n",
      "270 0.48568267273961185\n",
      "271 0.4671169498389093\n",
      "272 0.44926261507091736\n",
      "273 0.43209097815742503\n",
      "274 0.41558212007873874\n",
      "275 0.39971180745873747\n",
      "276 0.3844561120678798\n",
      "277 0.3697982706645862\n",
      "278 0.35569921559725853\n",
      "279 0.34213602775140217\n",
      "280 0.3290920305096632\n",
      "281 0.3165540280290975\n",
      "282 0.30449288870694735\n",
      "283 0.29290775378489375\n",
      "284 0.28175956916217876\n",
      "285 0.27103649367342286\n",
      "286 0.2607265436217573\n",
      "287 0.25081398246370584\n",
      "288 0.24127947390796545\n",
      "289 0.23212148315385397\n",
      "290 0.22330411128829653\n",
      "291 0.21482220218519543\n",
      "292 0.2066707310657996\n",
      "293 0.19882667955275163\n",
      "294 0.19128349641211306\n",
      "295 0.18403697675757108\n",
      "296 0.1770580977927536\n",
      "297 0.17034664981680564\n",
      "298 0.1638939755227865\n",
      "299 0.15768433452360267\n",
      "300 0.1517135035072995\n",
      "301 0.14597471226898578\n",
      "302 0.1404494016008516\n",
      "303 0.1351357801823107\n",
      "304 0.13002339617347763\n",
      "305 0.12510532058407864\n",
      "306 0.12037831891323633\n",
      "307 0.1158336192120934\n",
      "308 0.11145789853981417\n",
      "309 0.1072480837820681\n",
      "310 0.1031978204588609\n",
      "311 0.09930297196381545\n",
      "312 0.09555676242350328\n",
      "313 0.09195303096638645\n",
      "314 0.08848393997543189\n",
      "315 0.08514682582298656\n",
      "316 0.08193661365077445\n",
      "317 0.07884784910484574\n",
      "318 0.07587843616490197\n",
      "319 0.07302196852358782\n",
      "320 0.07027122315631619\n",
      "321 0.06762554134427154\n",
      "322 0.06507951702883678\n",
      "323 0.0626301744359117\n",
      "324 0.06027539962994403\n",
      "325 0.05800940060883875\n",
      "326 0.055827969067222316\n",
      "327 0.053729160467262865\n",
      "328 0.051709066942679675\n",
      "329 0.049766036970559396\n",
      "330 0.04789741065417756\n",
      "331 0.04609831515982507\n",
      "332 0.0443668759313258\n",
      "333 0.04270092236363205\n",
      "334 0.041098250115874105\n",
      "335 0.03955655011300152\n",
      "336 0.038073547864405186\n",
      "337 0.03664522096436448\n",
      "338 0.035270624058919633\n",
      "339 0.03394775504929093\n",
      "340 0.032675142696776224\n",
      "341 0.03145051891089559\n",
      "342 0.03027345955598417\n",
      "343 0.0291398432378115\n",
      "344 0.0280481137596762\n",
      "345 0.026997633055783017\n",
      "346 0.025986854495418567\n",
      "347 0.02501422144437984\n",
      "348 0.024078533755637257\n",
      "349 0.023177309544913656\n",
      "350 0.022310304662593896\n",
      "351 0.021476126357688113\n",
      "352 0.02067311589126157\n",
      "353 0.019900366086591036\n",
      "354 0.01915681380962128\n",
      "355 0.018440689740849326\n",
      "356 0.01775159871398657\n",
      "357 0.017088445118125226\n",
      "358 0.016450147505821966\n",
      "359 0.0158360337969494\n",
      "360 0.015245023855842591\n",
      "361 0.014676142073502488\n",
      "362 0.014128408471546804\n",
      "363 0.013601039477594144\n",
      "364 0.013093481204052156\n",
      "365 0.012605352620299137\n",
      "366 0.012135263543966148\n",
      "367 0.011682884539242619\n",
      "368 0.01124723021779795\n",
      "369 0.010827889321798543\n",
      "370 0.010424328077402945\n",
      "371 0.010036127191693781\n",
      "372 0.00966233459182985\n",
      "373 0.0093024271621014\n",
      "374 0.008955870059603951\n",
      "375 0.008622267811883228\n",
      "376 0.00830115812917202\n",
      "377 0.007992361866670556\n",
      "378 0.007694971343974557\n",
      "379 0.007408672419921354\n",
      "380 0.007133091370450822\n",
      "381 0.006867653986155626\n",
      "382 0.006612165130233047\n",
      "383 0.0063665350011783235\n",
      "384 0.0061297445699501334\n",
      "385 0.0059018303268206194\n",
      "386 0.0056823811976151675\n",
      "387 0.005471187981801691\n",
      "388 0.005267913978559068\n",
      "389 0.005072373141696275\n",
      "390 0.004883875400046252\n",
      "391 0.004702432044253385\n",
      "392 0.004527735995774372\n",
      "393 0.004359604870630203\n",
      "394 0.00419784020900015\n",
      "395 0.004042066940040017\n",
      "396 0.0038919935573338157\n",
      "397 0.003747582447869468\n",
      "398 0.0036085468473449423\n",
      "399 0.003474723015757915\n",
      "400 0.003345855964141407\n",
      "401 0.0032218145196940577\n",
      "402 0.0031023135708749512\n",
      "403 0.002987262903602777\n",
      "404 0.0028765122423975494\n",
      "405 0.0027698508321378377\n",
      "406 0.0026672227176626016\n",
      "407 0.002568385373096141\n",
      "408 0.002473262045403344\n",
      "409 0.0023816526135233443\n",
      "410 0.00229340201770602\n",
      "411 0.002208435844459381\n",
      "412 0.0021266934351930455\n",
      "413 0.0020479349872128275\n",
      "414 0.0019721227355730927\n",
      "415 0.0018991340699802562\n",
      "416 0.0018288477672936663\n",
      "417 0.0017611537593867077\n",
      "418 0.0016960401755969977\n",
      "419 0.0016332963695099905\n",
      "420 0.0015728548504740996\n",
      "421 0.0015146541253734844\n",
      "422 0.0014586300115441483\n",
      "423 0.0014046864966675704\n",
      "424 0.001352802889466507\n",
      "425 0.0013027787679490612\n",
      "426 0.0012546018065599678\n",
      "427 0.0012082275112890346\n",
      "428 0.0011635625632022075\n",
      "429 0.0011205902545614753\n",
      "430 0.001079208806670256\n",
      "431 0.001039332649420281\n",
      "432 0.0010009295907951726\n",
      "433 0.0009639474510423955\n",
      "434 0.0009283500343284622\n",
      "435 0.0008940793983182728\n",
      "436 0.0008610934039868285\n",
      "437 0.0008293062723376402\n",
      "438 0.0007986856716119398\n",
      "439 0.0007692094728506188\n",
      "440 0.0007408133359581591\n",
      "441 0.0007134788412781753\n",
      "442 0.0006871631477959846\n",
      "443 0.0006618099147748908\n",
      "444 0.0006373980595158853\n",
      "445 0.0006138801591894047\n",
      "446 0.000591234586904851\n",
      "447 0.0005694431957075967\n",
      "448 0.0005484546020809703\n",
      "449 0.0005282390284815084\n",
      "450 0.0005087615111398627\n",
      "451 0.0004900015500455706\n",
      "452 0.00047193774268797353\n",
      "453 0.00045455263267618086\n",
      "454 0.00043781004099094123\n",
      "455 0.0004216788170100061\n",
      "456 0.0004061462745272585\n",
      "457 0.000391183553392761\n",
      "458 0.00037677092639114426\n",
      "459 0.0003629065249602651\n",
      "460 0.00034954083851768646\n",
      "461 0.00033666569750811473\n",
      "462 0.000324266709318271\n",
      "463 0.00031232892678566835\n",
      "464 0.00030083272933272806\n",
      "465 0.0002897676126791939\n",
      "466 0.00027910539614911544\n",
      "467 0.00026883211335472577\n",
      "468 0.00025893831633141463\n",
      "469 0.00024941206465649334\n",
      "470 0.00024023455563469437\n",
      "471 0.00023140319959265602\n",
      "472 0.00022288870752120316\n",
      "473 0.00021469106096146614\n",
      "474 0.00020679474083971162\n",
      "475 0.00019918921141448862\n",
      "476 0.00019186989164772357\n",
      "477 0.00018482027855450328\n",
      "478 0.0001780259416366775\n",
      "479 0.00017148078755705742\n",
      "480 0.00016517682047411796\n",
      "481 0.0001591056557378119\n",
      "482 0.0001532584044901814\n",
      "483 0.00014763361765589013\n",
      "484 0.00014220745493094414\n",
      "485 0.00013698186085684753\n",
      "486 0.00013194991663737128\n",
      "487 0.00012710226463662346\n",
      "488 0.00012243488584945766\n",
      "489 0.00011794219243120993\n",
      "490 0.00011360970016226775\n",
      "491 0.00010943683347784096\n",
      "492 0.00010541826386856856\n",
      "493 0.00010154795312124619\n",
      "494 9.782033707297653e-05\n",
      "495 9.423243240691087e-05\n",
      "496 9.077479458798532e-05\n",
      "497 8.744423440362361e-05\n",
      "498 8.423538626121265e-05\n",
      "499 8.114339726433221e-05\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# implement the network using numpy\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension\n",
    "# H is hidden dimension; D_out is output dimension\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random. randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    #Compute and print loss\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradient of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weight\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "cuda:0\n",
      "99 222.92034912109375\n",
      "199 0.5531930923461914\n",
      "299 0.0025878818705677986\n",
      "399 0.00010113709868164733\n",
      "499 2.49585609708447e-05\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# use Tensors\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    \n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "99 1114.3544921875\n",
      "199 13.568634033203125\n",
      "299 0.24144330620765686\n",
      "399 0.0050233290530741215\n",
      "499 0.0002936195523943752\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Tensors and autograd\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    loss.backward()\n",
    "    \n",
    "    # Wrap in torch.no_grad() beacause weights have requires_grad=True, but we don't need to track this update in autograd\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "99 387.7276916503906\n",
      "199 1.9804493188858032\n",
      "299 0.016391541808843613\n",
      "399 0.00034282461274415255\n",
      "499 5.0839284085668623e-05\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Define new autograd functions\n",
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing torch.autograd.Function\n",
    "    and implementing the forward and backward passes which operate on Tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        :param ctx: is a context object that can be used to stash information for backward computation\n",
    "        :param input:  A Tensor containing the input \n",
    "        :return: A Tensor containing the output\n",
    "        You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # to apply our Function, we use Function.apply method. We alias this as 'relu'\n",
    "    relu = MyReLU.apply\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    loss.backward()\n",
    "    \n",
    "    # Wrap in torch.no_grad() beacause weights have requires_grad=True, but we don't need to track this update in autograd\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nn module\n",
    "import torch\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as out loss function\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute y_pred by passing x to the model. Module objects\n",
    "    # override __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module an d it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the lloss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # Zero the gradients before running the backward pass\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable \n",
    "    # parameterers of the model. Internally, the parameters of each Modules are stored\n",
    "    # in Tensors withe requires_grad=True, so this call will compute gradients for all learnable parameters in the model\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# optim Up to this point we have updated the weights of our models by manually mutating the\n",
    "# Tensors holding learnable parameters(with torch.no_grad() or .data to avoid tracking history in autograd\n",
    "# This is not a huge burden for simple optimization algorithms like stochastic gradient descent,\n",
    "# but in practice we often tran neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam,etc\n",
    "\n",
    "# The optim package in Pytorch abstracts the idea of an optimization algorithm and provides implementations\n",
    "# of commonly used optimization algorithms.\n",
    "\n",
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of \n",
    "# the model for us. Here we use Adam; the optim package contains many other \n",
    "# optimization algorithms. The first argument to the Adam constructor tells the \n",
    "# optimizer which Tensors it should update\n",
    "learning_rate = 1e-4 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables if will update (which are the learnable weights\n",
    "    # of the model). This is because by default, gradients are accumulated in buffers\n",
    "    # (i.e, not overwritten) whenever .backward() is called. Checkout docs of torch.autograd.backward for more details\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "99 666.1162109375\n",
      "199 648.470458984375\n",
      "299 631.3606567382812\n",
      "399 614.808349609375\n",
      "499 598.757568359375\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# PyTorch: Custom nn Modules\n",
    "\"\"\"\n",
    "Sometimes you will want to specify models that are more complex than a sequence of existing Moduels;\n",
    "for these cases you can define your own Modules by subclassing nn.Module and defining a forward which\n",
    "receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors\n",
    "\"\"\"\n",
    "import torch \n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them \n",
    "        as member variables\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "99 46.20490264892578\n",
      "199 9.365419387817383\n",
      "299 45.42475891113281\n",
      "399 17.987531661987305\n",
      "499 7.554983139038086\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Pytorch: Control Flow + Weight Sharing\n",
    "\"\"\"\n",
    "As an example of dynamic graphs and weights sharing, we implement a very strange model:\n",
    "a fully-connected ReLU network that on each forward pass chooses a random number between\n",
    "1 and 4 and uses that many hidden layers, reusing the same weights multiple times to compute \n",
    "the innermost hidden layers\n",
    "For this model we can normal Python flow control to implement the loop, and we can implement weight\n",
    "sharing among the innermost layers by simply reusing the same Module multiple times when defining the \n",
    "forward pass\n",
    "\"\"\"\n",
    "import random\n",
    "import torch\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use in the\n",
    "        forward pass \n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.hidden_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2 or 3\n",
    "        and reuse the middle_layer Module that many times to compute hidden layer representation\n",
    "        Since each forward pass bulids a dynamic computation graph, we can use normal Python \n",
    "        control-flow operators like loops or conditional statements when defining the forward pass\n",
    "        of the model.\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many times when defining\n",
    "        a computational graph. This is a big improvement from Lua Torch, where each Module could be used\n",
    "        only once\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.hidden_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "N, D_in, H, D_out = 64, 100, 1000, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-7851a9c0",
   "language": "python",
   "display_name": "PyCharm (test1)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}